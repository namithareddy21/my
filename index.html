<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Real-Time Object Detection</title>

  <!-- ONNX Runtime Web -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.19.0/dist/ort.min.js"></script>

  <style>
    *, ::before, ::after { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: radial-gradient(circle at top, #e0f2fe 0, #f8fafc 45%, #e2e8f0 100%);
      min-height: 100vh;
      color: #0f172a;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 24px;
    }
    h1 {
      letter-spacing: 0.2em;
      font-size: clamp(1.7rem, 4vw, 2.4rem);
      margin-bottom: 18px;
      color: #0f172a;
    }
    .buttons {
      display: flex;
      gap: 16px;
      margin-bottom: 24px;
    }
    .btn {
      padding: 10px 24px;
      border-radius: 999px;
      border: none;
      cursor: pointer;
      font-weight: 600;
      font-size: 0.95rem;
      letter-spacing: 0.04em;
      box-shadow: 0 12px 30px rgba(15, 23, 42, 0.25);
      transition: transform 0.18s ease, box-shadow 0.18s ease, background 0.18s ease;
    }
    .btn-start {
      background: linear-gradient(135deg, #22c55e, #4ade80);
      color: #ecfdf5;
    }
    .btn-stop {
      background: linear-gradient(135deg, #ef4444, #f97373);
      color: #fef2f2;
    }
    .btn:hover { transform: translateY(-1px); box-shadow: 0 16px 35px rgba(15, 23, 42, 0.3); }
    .btn:active { transform: translateY(0); box-shadow: 0 8px 20px rgba(15, 23, 42, 0.2); }

    .boxes {
      display: grid;
      grid-template-columns: repeat(2, minmax(0, 1fr));
      gap: 24px;
      width: min(1100px, 100%);
    }
    .box {
      background: rgba(255, 255, 255, 0.82);
      backdrop-filter: blur(20px);
      border-radius: 24px;
      padding: 18px;
      box-shadow: 0 18px 45px rgba(15, 23, 42, 0.25);
      border: 1px solid rgba(148, 163, 184, 0.4);
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 8px;
    }
    .box-title {
      font-size: 0.95rem;
      font-weight: 600;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: #64748b;
    }
    .video-wrapper {
      position: relative;
      width: 100%;
      padding-top: 70%;
      border-radius: 20px;
      overflow: hidden;
      background: #020617;
    }
    #video, #canvas {
      position: absolute;
      top: 0; left: 0;
      width: 100%; height: 100%;
      object-fit: cover;
    }
    #canvas {
      border-radius: 18px;
      border: 2px dashed rgba(34, 197, 94, 0.7);
    }
    .hint {
      margin-top: 16px;
      font-size: 0.85rem;
      color: #64748b;
    }
    @media (max-width: 768px) {
      .boxes { grid-template-columns: 1fr; }
    }
  </style>
</head>
<body>
  <h1>REAL-TIME OBJECT DETECTION</h1>

  <div class="buttons">
    <button id="startBtn" class="btn btn-start">Start Detection</button>
    <button id="stopBtn"  class="btn btn-stop">Stop Detection</button>
  </div>

  <div class="boxes">
    <div class="box">
      <div class="box-title">INPUT CAMERA</div>
      <div class="video-wrapper">
        <video id="video" autoplay muted playsinline></video>
      </div>
    </div>
    <div class="box">
      <div class="box-title">DETECTED OUTPUT</div>
      <div class="video-wrapper">
        <canvas id="canvas"></canvas>
      </div>
    </div>
  </div>

  <p class="hint">
    Hosted on Render. Camera and detection run entirely in your browser. Allow camera permission, then click <span style="font-weight:600;">Start Detection</span>.
  </p>

  <script>
    // COCO 80 class labels (short list here; extend if needed)
    const COCO_CLASSES = [
      "person","bicycle","car","motorcycle","airplane","bus","train","truck","boat","traffic light",
      "fire hydrant","stop sign","parking meter","bench","bird","cat","dog","horse","sheep","cow",
      "elephant","bear","zebra","giraffe","backpack","umbrella","handbag","tie","suitcase","frisbee",
      "skis","snowboard","sports ball","kite","baseball bat","baseball glove","skateboard","surfboard",
      "tennis racket","bottle","wine glass","cup","fork","knife","spoon","bowl","banana","apple",
      "sandwich","orange","broccoli","carrot","hot dog","pizza","donut","cake","chair","couch",
      "potted plant","bed","dining table","toilet","tv","laptop","mouse","remote","keyboard","cell phone",
      "microwave","oven","toaster","sink","refrigerator","book","clock","vase","scissors","teddy bear",
      "hair drier","toothbrush"
    ];

    const video  = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx    = canvas.getContext('2d');
    const startBtn = document.getElementById('startBtn');
    const stopBtn  = document.getElementById('stopBtn');

    let stream = null;
    let session = null;
    let running = false;
    let rafId = null;

    async function loadModel() {
      if (session) return session;
      ort.env.wasm.numThreads = 1;
      // yolov8n.onnx must be in same folder (Render static files)
      session = await ort.InferenceSession.create('yolov8n.onnx', {
        executionProviders: ['wasm']
      });
      return session;
    }

    async function startCamera() {
      if (stream) return;
      stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 640, height: 480 },
        audio: false
      });
      video.srcObject = stream;

      await new Promise(res => { video.onloadedmetadata = () => res(); });
      canvas.width  = video.videoWidth;
      canvas.height = video.videoHeight;
    }

    function stopCamera() {
      if (stream) {
        stream.getTracks().forEach(t => t.stop());
        stream = null;
      }
      video.srcObject = null;
    }

    function preprocess() {
      const w = canvas.width;
      const h = canvas.height;
      ctx.drawImage(video, 0, 0, w, h);
      const imgData = ctx.getImageData(0, 0, w, h);

      const inputW = 640;
      const inputH = 640;
      const data = imgData.data;
      const resized = new Uint8ClampedArray(inputW * inputH * 4);

      // simple nearest-neighbor resize
      for (let y = 0; y < inputH; y++) {
        for (let x = 0; x < inputW; x++) {
          const srcX = Math.floor(x * w / inputW);
          const srcY = Math.floor(y * h / inputH);
          const srcIdx = (srcY * w + srcX) * 4;
          const dstIdx = (y * inputW + x) * 4;
          resized[dstIdx]     = data[srcIdx];
          resized[dstIdx + 1] = data[srcIdx + 1];
          resized[dstIdx + 2] = data[srcIdx + 2];
          resized[dstIdx + 3] = 255;
        }
      }

      const float32Data = new Float32Array(3 * inputW * inputH);
      let p = 0;
      for (let i = 0; i < resized.length; i += 4) {
        const r = resized[i]     / 255.0;
        const g = resized[i + 1] / 255.0;
        const b = resized[i + 2] / 255.0;
        float32Data[p]                   = r;
        float32Data[p + inputW * inputH] = g;
        float32Data[p + 2 * inputW * inputH] = b;
        p++;
      }
      return new ort.Tensor('float32', float32Data, [1, 3, inputH, inputW]);
    }

    function nonMaxSuppression(boxes, scores, iouThreshold) {
      const idxs = scores
        .map((s, i) => [s, i])
        .sort((a, b) => b[0] - a[0])
        .map(x => x[1]);

      const selected = [];
      while (idxs.length > 0) {
        const current = idxs.shift();
        selected.push(current);

        const rest = [];
        for (const idx of idxs) {
          const iou = boxIou(boxes[current], boxes[idx]);
          if (iou < iouThreshold) rest.push(idx);
        }
        idxs.splice(0, idxs.length, ...rest);
      }
      return selected;
    }

    function boxIou(a, b) {
      const [x1, y1, x2, y2] = a;
      const [x1b, y1b, x2b, y2b] = b;
      const interX1 = Math.max(x1, x1b);
      const interY1 = Math.max(y1, y1b);
      const interX2 = Math.min(x2, x2b);
      const interY2 = Math.min(y2, y2b);
      const interW = Math.max(0, interX2 - interX1);
      const interH = Math.max(0, interY2 - interY1);
      const interArea = interW * interH;
      const areaA = (x2 - x1) * (y2 - y1);
      const areaB = (x2b - x1b) * (y2b - y1b);
      return interArea / (areaA + areaB - interArea + 1e-6);
    }

    async function detect() {
      const sess = await loadModel();
      const inputTensor = preprocess();

      const outputs = await sess.run({ images: inputTensor });
      const output = outputs[Object.keys(outputs)[0]]; // [1, 84, 8400]
      const data = output.data;
      const numClasses = COCO_CLASSES.length;
      const numBoxes = data.length / (numClasses + 4);

      const boxes = [];
      const scores = [];
      const classIds = [];

      const imgW = canvas.width;
      const imgH = canvas.height;

      const confThreshold = 0.4;

      for (let i = 0; i < numBoxes; i++) {
        const offset = i * (numClasses + 4);
        const cx = data[offset];
        const cy = data[offset + 1];
        const w  = data[offset + 2];
        const h  = data[offset + 3];

        let maxScore = -Infinity;
        let clsId = -1;
        for (let c = 0; c < numClasses; c++) {
          const score = data[offset + 4 + c];
          if (score > maxScore) {
            maxScore = score;
            clsId = c;
          }
        }

        const score = 1 / (1 + Math.exp(-maxScore)); // sigmoid

        if (score < confThreshold) continue;

        const x1 = (cx - w / 2) * imgW / 640;
        const y1 = (cy - h / 2) * imgH / 640;
        const x2 = (cx + w / 2) * imgW / 640;
        const y2 = (cy + h / 2) * imgH / 640;

        boxes.push([x1, y1, x2, y2]);
        scores.push(score);
        classIds.push(clsId);
      }

      const keep = nonMaxSuppression(boxes, scores, 0.45);

      return keep.map(i => {
        const [x1, y1, x2, y2] = boxes[i];
        const w = x2 - x1;
        const h = y2 - y1;
        return {
          x1, y1, x2, y2,
          w, h,
          score: scores[i],
          cls: classIds[i],
          label: COCO_CLASSES[classIds[i]] || 'object'
        };
      });
    }

    async function loop() {
      if (!running) return;

      // Draw current frame first
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

      try {
        const detections = await detect();

        // Draw boxes on top
        for (const det of detections) {
          const x1 = det.x1;
          const y1 = det.y1;
          const x2 = det.x2;
          const y2 = det.y2;

          ctx.strokeStyle = '#22c55e';
          ctx.lineWidth = 2;
          ctx.beginPath();
          ctx.rect(x1, y1, x2 - x1, y2 - y1);
          ctx.stroke();

          const text = `${det.label} ${(det.score * 100).toFixed(1)}% | ${Math.round(det.w)}x${Math.round(det.h)} px`;
          ctx.font = '14px system-ui';
          const tw = ctx.measureText(text).width;
          const th = 16;

          ctx.fillStyle = 'rgba(34,197,94,0.9)';
          ctx.fillRect(x1, y1 - th - 4, tw + 8, th + 4);

          ctx.fillStyle = '#ffffff';
          ctx.fillText(text, x1 + 4, y1 - 8);
        }
      } catch (e) {
        console.error('Detection error:', e);
      }

      rafId = requestAnimationFrame(loop);
    }

    startBtn.onclick = async () => {
      if (running) return;
      try {
        await startCamera();
        await loadModel();
        running = true;
        loop();
      } catch (e) {
        alert('Error starting detection: ' + e.message);
        console.error(e);
      }
    };

    stopBtn.onclick = () => {
      running = false;
      if (rafId) cancelAnimationFrame(rafId);
      stopCamera();
      ctx.clearRect(0, 0, canvas.width, canvas.height);
    };
  </script>
</body>
</html>
